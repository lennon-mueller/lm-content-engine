# Course 6: Capstone – Responsible AI Solution Lab

## Course Purpose
Provide a culminating, end-to-end project where learners design, implement, and evaluate an AI solution using Google tools, demonstrating responsible practices from ideation to deployment.

## Target Skills
- Frame a problem, gather/prepare data, and select an appropriate modeling approach.
- Build, evaluate, and iterate on a model with clear documentation and reproducibility.
- Deploy a minimal viable pipeline or demo and implement monitoring and safety checks.
- Communicate findings, risks, and mitigations to stakeholders with humility and clarity.

## Course Architecture (3 phases mapped to WWHAA per milestone)
1. **Proposal and Plan (Week 1)**  
   - Why: Planning reduces rework and aligns teams; hook with stakeholder confidence.  
   - What: Problem statement, success metrics, risks, data contracts, responsible AI commitments.  
   - How: Templates for proposal, risk log, and experiment plan; mentor video on scope control.  
   - Apply: Learners submit a proposal and data readiness checklist.  
   - Assess: Mentor/peer review with rubric; short quiz on scope and ethics.  
   - Playful context: Dolphin navigation chart as metaphor for setting a clear course.

2. **Build and Validate (Weeks 2–3)**  
   - Why: Iteration with checkpoints protects quality; hook on shipping value safely.  
   - What: Data prep, feature engineering, baseline, tuning, fairness/robustness checks.  
   - How: Colab/Vertex AI workspace with starter pipeline; weekly stand-up reflections.  
   - Apply: Learners build model, log experiments, and produce evaluation artifacts (including model card draft).  
   - Assess: Auto-graded checkpoints for reproducibility; rubric-led mid-project review; quiz referencing review materials.  
   - Playful context: Dolphin pod collaboration during hunts to emphasize iterative teamwork.

3. **Deploy, Monitor, and Present (Weeks 4–5)**  
   - Why: Responsible release builds trust; hook on avoiding negative user impact.  
   - What: Deployment options (demo endpoint or batch job), monitoring plans, rollback triggers, stakeholder storytelling.  
   - How: Learners deploy to a managed endpoint or batch pipeline, set alerts, and run a safety evaluation.  
   - Apply: Final deliverables: working demo/pipeline link, monitoring plan, updated model card, stakeholder presentation.  
   - Assess: Final rubric-based evaluation; viva-style presentation with Q&A; reflective survey.  
   - Playful context: Dolphin safety drills before long journeys to reinforce monitoring and rollback.

## Assessments and Practice
- Rubric-driven reviews at proposal, mid-project, and final presentation stages.
- Quizzes with variations tied to responsible AI and operational excellence checkpoints.
- Reflection prompts to capture lessons learned and next steps.

## Tools and Logistics
- Tools: Colab, BigQuery/Cloud Storage, Vertex AI (training, endpoints or batch), Docs/Slides templates.  
- Accessibility: captions, alt text, accessible presentation templates; low-bandwidth options for demos.
